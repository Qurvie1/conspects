Раньше мы что делали
$L = L_{1} + L_{2}$ - прямая сумма

$x = x_{1} + x_{2}$, где $x_{1} \in L_{1}$, $x_{2} \in L_{2}$

$x_{1}$ - проекция на $L_{1}$ вдоль $L_{2}$.

Теперь у нас:

$L \subset E$, $E$ - [[conspects/linal/Евклидовы пространства|евклидово пространство]].

И возьмём $L^{\perp}$ - [[Ортогональное дополнение|ортогональное дополнение]].

Тогда $E = L + L^{\perp}$ - прямая сумма.

Тогда аналогично можно разложить $x = x_{\parallel} + x_{\perp}$, где $x_{\parallel}$ - **ортогональная проекция** на $L$.

Решим задачу $N4$.

$L = <a_{1}, a_{2}>$

$a_{1} = \begin{pmatrix}1 & 2 & 1 & 0\end{pmatrix}^{T}$
$a_{2} = \begin{pmatrix}2 & 1 & 4 & 5\end{pmatrix}^{T}$

Нужно найти ортогональную проекцию $x = \begin{pmatrix}2 & -5 & 4 & -3\end{pmatrix}^{T}$

0 способ: найдём ортогональное дополнение, пусть $L^{\perp} = <b_{1}, b_{2}>$

Тогда $x = \alpha_{1} a_{1} + \alpha_{2} a_{2} + \beta_{1} b_{1} + \beta_{2} b_{2}$, тогда то, что с $a$ и будет ортогональной проекцией вдоль.

**1 способ:**

Разобьём то, что выше на $x_{\parallel}, x_{\perp}$

Но тогда $x_{\perp}$ - перпендикулярно всем a.

$$
\begin{cases}
(x_{\perp}, a_{1}) = 0  \\
(x_{\perp}, a_{2}) = 0
\end{cases} \iff \begin{cases}
(x_{1}, a_{1}) - \alpha_{1}(a_{1}a_{1}) - \alpha_{2}(a_{1}, a_{2}) = 0 \\
(x_{1}, a_{2}) - \alpha_{1}(a_{1}a_{2}) - \alpha_{2}(a_{2}, a_{2}) = 0

\end{cases}
$$
Или
$$
\begin{pmatrix}(a_{1},a_{1}) & (a_{1},a_{2}) \\ (a_{1},a_{2}) & (a_{2},a_{2})\end{pmatrix}\begin{pmatrix}\alpha_{1} \\ \alpha_{2} \end{pmatrix} = \begin{pmatrix}(x_{1}, a_{1}) \\ (x_{2}, a_{2})\end{pmatrix}
$$
Слева у нас по сути [[conspects/linal/Евклидовы пространства|матрица Грама]].

И мы переходим к решению [[Системы линейных уравнений|системы]].

Получаем $\alpha_{1} = -2, \alpha_{2} = 1$

Тогда $x_{\parallel} = -2\begin{pmatrix}1 \\ 2 \\ 1 \\ 0\end{pmatrix} + 1\begin{pmatrix}2 \\ 1 \\ 4 \\ 5\end{pmatrix} = \begin{pmatrix}0 \\ -3 \\ 2 \\ 5\end{pmatrix}$

**2 способ:**

Пусть $b_{1}, ..., b_{n}$ - ортогональные. И $x$ как-то раскладывается.

И мы хотим найти ортогональную проекцию на первых $k$ векторов, то есть $x_{\parallel} = \beta_{1}b_{1} + ... + \beta_{k}b_{k}$

Тогда если мы будем скалярно умножать на вектора $b_{1}, ..., b_{k}$, то у нас будут зануляться все (в силу [[Ортогональные векторы|ортогональности]]).

При этом останется только сам вектор

Поэтому $(x, b_{i}) = \beta_{i}(b_{i}, b_{i})$ - получаем систему из $k$ уравнений, которую можем решить.

$\beta_{k} = \frac{(x, b_{k})}{(b_{k}, b_{k})}$ - так получаем коэффициенты.

Но верно только для ортогонального базиса. А сделать ортогональный можно с помощью [[Процесс ортогонализации Грама-Шмидта|ортогонализации]].

Решим задачу теперь:

$b_{1} = a_{1}$

$b_{2} = a_{2} - \frac{(b_{1}, a_{2})}{(b_{1}, b_{1})}b_{1} = \frac{1}{3}\begin{pmatrix}2 \\ -5 \\ 8 \\ 15\end{pmatrix}$
Тогда:
$$
x_{\parallel} = \frac{(x_{1}, b_{1})}{(b_{1}, b_{1})}b_{1} + \frac{(x_{1}, b_{2})}{(b_{2}, b_{2})}b_{2}
$$
Считаем и получаем тот же ответ.
