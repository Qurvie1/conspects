1.1.1
ну например, если нам нужно как-то распределить задачи в зависимости от их сложности, тогда нужно упорядочить их по сложности
или например мы хотим обменять крипту в обменнике, но их бывает очень много, нужно их отсортировать по разным параметрам.
1.1.2.
Память
1.1.3. Двоичная куча - плюсы - возможность находить максимум и минимум, минусы - нет возможности взаимодействовать с элементами по значению, а не по ключу + не можем найти конкретное значение.
1.1.4.
Ну по сути, общее в том, что нам нужно найти кратчайшую дорогу, сложность в том, что ещё нужно вернуться обратно, как в случае коммивояжера.
1.1.5
допустим, мы хотим кинуть камень на какое-то расстояние, нам же необязательно знать именно как его кинуть на максимальное расстояние, нас интересует расстояние >= нужному, поэтому нас не интересует точное значение.
точность: нахождение корня уравнения 1 + x = 2
1.2.1 ну вот например, переводчик, нам нужно найти похожее значение слова, тут удобно использовать сплей-дерево, потому что логично, что нас не интересует какое-то тупое слово встречающееся 1 раз на миллион, нас интересуют в первую очередь слова, который близки к началу (то есть которые встречаются чаше), тут нам и нужно сплей-дерево.
1.2.2-3 - решить неравенство
Задачи 1.1
f(n) = t, t приведено к микросекундам (10^-6), отсюда находим.
2.1.
insertion sort - находим место, куда вставить ту или иную карту
2.1-1.
31, 41, 59, 26, 41, 58
insertion sort
просто берём следующий элемент и сдвигаем, если нужно
типа
31, 41, 59, потом оп, 26 уже меньше, вставляем перед 31
26, 31, 41, 59, потом аналогично ищем куда вставить 41 и 58, вставляем, всё окей
2.1-2
просто по сути делать это только вставляя сначала максимум
2.1-3
смех (ну у нас последовательность первых j чисел она проверена уже)
2.1-4
ну просто короче складываем как в школе в столбик массивы
2.2.1
O(n^3)
2.2.2
псевдокод: ищем минимум постоянно
для n - 1 очев, в среднем и худшем O(n^2)
2.2.3
В наихудшем случае мы проходим просто весь массив, в среднем мы проходим половину массива, поэтому O(n / 2) = O(n)
2.2.4 
Предпосчитать значения
1.3.1 Разделяй и властвуй
задача разбивается на подзадачи, а они решаются рекурсивно 
Ну отсюда идея MergeSort
Мы просто делим массив пока он не будет единичной длины
ну а потом мерджим
время работы мерджа O(n + m)
как оценить алгоритм, он же рекурсивный!
ну пусть мы разбиваем задачу из n элементов на a меньших в b раз, при этом разбиение тратит D(n), а соединение C(n)
Тогда
T(n) = aT(n / b) + D(n) + C(n)
Теперь сделаем анал мерджсорта
T(n) = 2T(n / 2) + O(n / 2) или O(1), если n = 1
Почему так? Разбиваем на пп постоянно + само разбиение за O(1) + O(n) - слияние (ну типа мы сравниваем эти штуки)

1.3.1 - всё просто - делим:
3 .. 41 .. 52 .. 26 .. 38 .. 57 .. 9 .. 49
3 41, 26 52, 38 57, 9 49
3 26 41 52, 9 38 49 57
3 9 26 .. (ну соединили короче)

1.3.2
Merge(A, p, q, r)
полуинтервальная логика
(p, q] и (q, r]
как мы это делаем?
2 указателя i -> p и j -> q
пока у нас оба указателя меньше длины массива, то мы сравниваем оба
что делаем?
сравниваем первые 2 - если первый больше, то берём из второго массива + увеличиваем указатель на элемент второго массива и так далее
когда вышли за границу одного из массива, то просто докидываем оставшиеся
1.3.3 оценка (2):
T(n) = n + 2(2T(n/4) + n / 2) = n + 2(2(2T(n / 8) + n / 4) + n / 2) = k * n + 2^kT (1)
ну а поскольку k = log2n, то
n * log2n + 2n = n (log2n + 2) = n log2n + log2 4 = nlog24n
1.3.4
ну по сути, типа это
T(n) = T(n - 1) + (n - 1)
1.3.5
потому что делим на пополам
1.3.6
ну вообще как будто бы нет, то есть мы действительно можем улучшить время работы алгоритма, но проблема в том, что основная затрата по времени идёт именно на вставку саму, потому что нам нужно же ещё сдвинуть элементы, поэтому не получится сделать nlogn
1.3.7
сортируем мерджсортом, потом 2 указателя - на начало и на конец, если сумма больше, то двигаем правый указатель влево, меньше - левый вправо, тогда если у нас никогда сумма не равна (то есть указатели уже встретились, а мы не нашли суммы равной), то нельзя представить
асимптотика очевидна

Задачи:
2.1
a) ну просто у сортировки вставками асимптотика theta(k^2), а так как их всего n / k, то theta (nk)
б) идея из дз вообще, пронумеруем короче их:
1, 2, ..., n/k
в чём идея
будем мерджить:
1 - 2, 3 - 4, ...,
а потом попарно эти
1 - 2 - 3 - 4, ...
в чём идея? количество последовательностей, которые мы мерджим всегда уменьшается в 2 раза, поэтому там как раз log(n / k), при этом мы как раз мерджим ровно n элементов
в) по сути нужно решить уравнение
nk + nlg(n/k) = cnlgn

k + lg(n / k) = clgn

2^k * n / k = 2^clgn = n^c
2^k / k = n^(c - 1)
г) на глаз (???)
2.2
а) +- очев, так как если 1 > 2, то мы где-то не выполнили swap, а такого не может быть
в, б) справа от элемента элементы >= этого, иначе мы бы не делали своп. тогда в целом всё становится очевидно на самом деле, так как для первого элемента верно, для второго верно и так далее, то есть это неравенство реально доказывается.
г) квадрат всегда
2.3
а) линия
б) вообще тут квадрат скорее всего, потому что мы сначала идём циклом по n, а потом 1, 2, 3 и так далее
в сумме мы получается делаем 1 + 2 + 3 + ... + n действий = theta(n^2) (потому что х в степень 1, 2, ... 3)
в) хз че тут происходит если честно
г) очев
2.4
а) очев
б) по убыванию если массив расположен, тогда инверсий n - 1 + n - 2 + ... = n(n + 1) / 2
в) ну вообще, идея в чём, когда мы делаем сортировку вставками, то мы делаем на самом деле n проверок + k обменов (где k - число инверсий)
г) уже было

3.1.1.
ну тут очевидно, потому что нас всегда интересует только максимальная степень в этих обозначениях, так как можно записать:
f(n) = n^a + theta(n^(a - 1))
поэтому +- очевидно,
3.1.2.
ну нас опять же интересует только наибольшая степень, потому что всё остальное асимптотически меньше и это просто в ошку запишется
3.1.3
ну просто это странно из определения асимптотических обозначений
O(n^2) - это значит, что верхняя границы времени работы алгоритма оценивается n^2, тогда в целом, правильнее было бы сказать омега(n^2).
3.1.4
2^n + 1 = 2 * 2^n <= c * 2^n => c >= 2
При c >= 2 и n >= 1 оценка будет верна, поэтому всё окей
2^2n <= c * 2^n
2^n * 2^n <= c * 2^n
2^n <= c
что верно не для всех n
ну то есть если предположить, что существует n0 и c, что при всех n >= n0 это верно, то придём просто к противоречию, так как слева бесконечно растущая функция, а справа просто константа, поэтому функция перевалит константу
3.1.5
базовый матанчик какой-то
3.1.6
в одну сторону: из определения theta
c1g(n) <= f(n) <= c2g(n)
поэтому в действительности:
c1g(n) <= f(n) - то есть в наилучшем случае его время работы как раз c1g(n) (или омега g(n))
в наихудшем понятно
в другую сторону просто переходим от 2 неравенств к цепочке неравенств
3.1.7
ну предположим противное, пусть существует f(n) принадлежащее этому м-ву
тогда с одной стороны:
зафиксируем какие-то c1 и c2 для ошки и омежки соответственно
тогда по определению o и w, если взять n > max(n1, n2):
f(n) < c1g(n)
с другой стороны
f(n) > c2g(n)
поскольку по определению малых это выполняется для любых констант, то можно взять константы равные
тогда у нас с одной стороны знак >, с другой <, противоречие
3.1.8
ну просто ещё одна переменная добавляется

3.2.1
ну очевидно, лень проверять, просто по определению в кванторах можно
3.2.2 
база на свойства лога
3.2.3.
ну просто можно в явном виде предъвить такое n, если заюзать формулу Стирлинга
3.2.4
первая вроде как нет, а вот вторая вроде да
понять можно в целом из формулы Стирлинга (ну если чисто подставить и посмотреть че происходит)
3.2.5
вторая больше, потому что первая практически жёстко сразу обрезает число
3.2.6
решить уравнение
3.2.7.
эээ
3.2.8
определение теты
Задачи
3.1.
На самом деле, мы когда считаем пределы f(n)/g(n), где сверху и снизу стоят полиномы, то мы считаем в любом случае именно пределы
(в случае малых)
в случае равных нам достаточно просто по определению расписать
3.2 - 3.6 - хуйня какая-то

4.1.1
просто на сравнениях работает, максимум из отрицательных подмассивах
4.1.2
просто первый фор - начало подмассива
второй фор - конец подмассива
считаем сумму явно

